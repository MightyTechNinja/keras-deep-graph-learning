{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Keras Deep Learning on Graphs (Keras-DGL)\n\n\nThe aim of this keras extension is to provide Sequential and Functional API for performing deep learning tasks on graphs. Specifically, Keras-DGL provides implementation for these particular type of layers, \n\n\n\n\nGraph Convolutional Neural Networks (GraphCNN).\n\n\nGraph Attention Convolutional Neural Networks (GraphAttentionCNN). \n\n\nGraph Convolutional Recurrent Neural Networks (GraphConvLSTM). \n\n\nGraph Capsule Convolutional Recurrent Neural Networks (GraphCapsuleCNN) TBD. \n\n\nGraph Message Passing Neural Networks (GraphNeuralNetworks) TBD. \n\n\nKeras-DGL also contains implementation of various graph convolutional filters TBD.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-keras-deep-learning-on-graphs-keras-dgl", 
            "text": "The aim of this keras extension is to provide Sequential and Functional API for performing deep learning tasks on graphs. Specifically, Keras-DGL provides implementation for these particular type of layers,    Graph Convolutional Neural Networks (GraphCNN).  Graph Attention Convolutional Neural Networks (GraphAttentionCNN).   Graph Convolutional Recurrent Neural Networks (GraphConvLSTM).   Graph Capsule Convolutional Recurrent Neural Networks (GraphCapsuleCNN) TBD.   Graph Message Passing Neural Networks (GraphNeuralNetworks) TBD.   Keras-DGL also contains implementation of various graph convolutional filters TBD.", 
            "title": "Welcome to Keras Deep Learning on Graphs (Keras-DGL)"
        }, 
        {
            "location": "/Layers/Convolution/graph_conv_layer/", 
            "text": "[source]\n\n\nGraphCNN\n\n\nGraphCNN(output_dim, num_filters, graph_conv_filters,  activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nGraphCNN layer assumes a fixed input graph structure which is passed as a layer argument. As a result, the input order of graph nodes are fixed for the model and should match the nodes order in inputs. Also, graph structure can not be changed once the model is compiled. This choice enable us to use Keras Sequential API but comes with some constraints (for instance shuffling is not  possible anymore in-or-after each epoch). See further \nremarks below\n about this specific choice.\n\n\nArguments\n\n\n\n\noutput_dim\n: Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).\n\n\nnum_filters\n: Positive integer, number of graph filters used for constructing  \ngraph_conv_filters\n input.\n\n\ngraph_conv_filters\n input as a 2D tensor with shape: \n(num_filters*num_graph_nodes, num_graph_nodes)\n\n\nnum_filters\n is different number of graph convolution filters to be applied on graph. For instance \nnum_filters\n could be power of graph Laplacian. Here list of graph convolutional matrices are stacked along second-last axis.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shapes\n\n\n\n\n2D tensor with shape: \n(num_graph_nodes, input_dim)\n representing graph node input feature matrix.\n\n\n\n\nOutput shape\n\n\n\n\n2D tensor with shape: \n(num_graph_nodes, output_dim)\n representing convoluted output graph node embedding (or signal) matrix.\n\n\n\n\n[source]\n\n\nExample 1\n: Graph Semi-Supervised Learning (or Node Classification)\n\n\n# A sample code for applying GraphCNN layer to perform node classification. \n# See examples/gcnn_node_classification_example.py for complete code.\n\nfrom keras_dgl.layers import GraphCNN\n\nmodel = Sequential()\nmodel.add(GraphCNN(16, 2, graph_conv_filters, input_shape=(X.shape[1],), activation='elu', kernel_regularizer=l2(5e-4)))\nmodel.add(Dropout(0.2))\nmodel.add(GraphCNN(Y.shape[1], 2, graph_conv_filters, kernel_regularizer=l2(5e-4)))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['acc'])\nmodel.fit(X, Y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=500, shuffle=False, verbose=0)\n\n\n\n\n[source]\n\n\nExample 2\n: Graph Edge Convolution for Node Classification\n\n\n# A sample code for applying GraphCNN layer while taking edge features into account to perform node label classification. \n# For edge convolution all we need is to provide a graph_conv_filters which contains (stack) adjacency matrices corresponding to each edge features. See note below on example2.\n# See graphcnn_example2.py for complete code.\n\nfrom keras_dgl.layers import GraphCNN\n\nmodel = Sequential()\nmodel.add(GraphCNN(16, 2, graph_conv_filters, activation='elu'))\nmodel.add(Dropout(0.2))\nmodel.add(GraphCNN(Y.shape[1], 2, graph_conv_filters))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.fit(X, Y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=500, shuffle=False)\n\n\n\n\nNote on Example 2:  Equation \n1\n in the paper (see reference [3]) can be written as \nY=\\sum\\limits_{s=0}^{S}A^{(s)}X\\theta^{(s)}\n. This is defined as graph edge convolution. All we have to do is stack \nA^{(s)}\n and feed to GraphCNN layer to perform graph edge convolution.\n\n\n\n\n[source]\n\n\nMutliGraphCNN\n\n\nMutliGraphCNN(output_dim, num_filters, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nMutliGraphCNN assumes that the number of nodes for each graph in the dataset is same. For graph with arbitrary size, one can simply append appropriate zero rows or columns in adjacency matrix (and node feature matrix) based on max graph size in the dataset to achieve this uniformity.\n\n\nArguments\n\n\n\n\noutput_dim\n: Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).\n\n\nnum_filters\n: Positive integer, number of graph filters used for constructing  \ngraph_conv_filters\n input.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shapes\n\n\n\n\ngraph node feature matrix\n input as a 3D tensor with shape: \n(batch_size, num_graph_nodes, input_dim)\n corresponding to graph node input feature matrix for each graph.\n\n\ngraph_conv_filters\n input as a 3D tensor with shape: \n(batch_size, num_filters*num_graph_nodes, num_graph_nodes)\n \n\n\nnum_filters\n is different number of graph convolution filters to be applied on graph. For instance \nnum_filters\n could be power of graph Laplacian.\n\n\n\n\nOutput shape\n\n\n\n\n3D tensor with shape: \n(batch_size, num_graph_nodes, output_dim)\n representing convoluted output graph node embedding matrix for each graph in batch size.\n\n\n\n\n[source]\n\n\nExample 3\n: Graph Classification\n\n\n# See multi_gcnn_graph_classification_example.py for complete code.\n\nfrom keras_dgl.layers import MultiGraphCNN\n\nX_shape = Input(shape=(X.shape[1], X.shape[2]))\ngraph_conv_filters_shape = Input(shape=(graph_conv_filters.shape[1], graph_conv_filters.shape[2]))\n\noutput = MultiGraphCNN(100, num_filters, activation='elu')([X_shape, graph_conv_filters_shape])\noutput = Dropout(0.2)(output)\noutput = MultiGraphCNN(100, num_filters, activation='elu')([output, graph_conv_filters_shape])\noutput = Dropout(0.2)(output)\noutput = Lambda(lambda x: K.mean(x, axis=1))(output)  # adding a node invariant layer to make sure output does not depends upon the node order in a graph.\noutput = Dense(Y.shape[1])(output)\noutput = Activation('softmax')(output)\n\nnb_epochs = 200\nbatch_size = 169\n\nmodel = Model(inputs=[X_shape, graph_conv_filters_shape], outputs=output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.fit([X, graph_conv_filters], Y, batch_size=batch_size, validation_split=0.1, epochs=nb_epochs, shuffle=True, verbose=1)\n\n\n\n\n\n\nRemarks\n\n\nWhy pass graph_conv_filters as a layer argument and not as an input in GraphCNN?\n\nThe problem lies with keras multi-input functional API. It requires --- all input arrays (x) should have the same number of samples i.e.,  all inputs first dimension axis should be same. In special cases the first dimension of inputs could be same, for example check out Kipf .et.al.  keras implementation \n[source]\n. But in cases such as a graph recurrent neural networks this does not hold true.\n\n\nWhy pass graph_conv_filters as 2D tensor of this specific format?\n\nPassing  graph_conv_filters input as a 2D tensor with shape: \n(K*num_graph_nodes, num_graph_nodes)\n cut down few number of tensor computation operations.\n\n\nReferences\n: \n\n[1] Kipf, Thomas N., and Max Welling. \"Semi-supervised classification with graph convolutional networks.\" arXiv preprint arXiv:1609.02907 (2016). \n\n[2] Defferrard, Micha\u00ebl, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional neural networks on graphs with fast localized spectral filtering.\" In Advances in Neural Information Processing Systems, pp. 3844-3852. 2016. \n\n[3] Simonovsky, Martin, and Nikos Komodakis. \"Dynamic edge-conditioned filters in convolutional neural networks on graphs.\" In Proc. CVPR. 2017.", 
            "title": "Graph Convolutional Layers"
        }, 
        {
            "location": "/Layers/Convolution/graph_conv_layer/#graphcnn", 
            "text": "GraphCNN(output_dim, num_filters, graph_conv_filters,  activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  GraphCNN layer assumes a fixed input graph structure which is passed as a layer argument. As a result, the input order of graph nodes are fixed for the model and should match the nodes order in inputs. Also, graph structure can not be changed once the model is compiled. This choice enable us to use Keras Sequential API but comes with some constraints (for instance shuffling is not  possible anymore in-or-after each epoch). See further  remarks below  about this specific choice.  Arguments   output_dim : Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).  num_filters : Positive integer, number of graph filters used for constructing   graph_conv_filters  input.  graph_conv_filters  input as a 2D tensor with shape:  (num_filters*num_graph_nodes, num_graph_nodes)  num_filters  is different number of graph convolution filters to be applied on graph. For instance  num_filters  could be power of graph Laplacian. Here list of graph convolutional matrices are stacked along second-last axis.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shapes   2D tensor with shape:  (num_graph_nodes, input_dim)  representing graph node input feature matrix.   Output shape   2D tensor with shape:  (num_graph_nodes, output_dim)  representing convoluted output graph node embedding (or signal) matrix.   [source]", 
            "title": "GraphCNN"
        }, 
        {
            "location": "/Layers/Convolution/graph_conv_layer/#example-1-graph-semi-supervised-learning-or-node-classification", 
            "text": "# A sample code for applying GraphCNN layer to perform node classification. \n# See examples/gcnn_node_classification_example.py for complete code.\n\nfrom keras_dgl.layers import GraphCNN\n\nmodel = Sequential()\nmodel.add(GraphCNN(16, 2, graph_conv_filters, input_shape=(X.shape[1],), activation='elu', kernel_regularizer=l2(5e-4)))\nmodel.add(Dropout(0.2))\nmodel.add(GraphCNN(Y.shape[1], 2, graph_conv_filters, kernel_regularizer=l2(5e-4)))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01), metrics=['acc'])\nmodel.fit(X, Y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=500, shuffle=False, verbose=0)  [source]", 
            "title": "Example 1: Graph Semi-Supervised Learning (or Node Classification)"
        }, 
        {
            "location": "/Layers/Convolution/graph_conv_layer/#example-2-graph-edge-convolution-for-node-classification", 
            "text": "# A sample code for applying GraphCNN layer while taking edge features into account to perform node label classification. \n# For edge convolution all we need is to provide a graph_conv_filters which contains (stack) adjacency matrices corresponding to each edge features. See note below on example2.\n# See graphcnn_example2.py for complete code.\n\nfrom keras_dgl.layers import GraphCNN\n\nmodel = Sequential()\nmodel.add(GraphCNN(16, 2, graph_conv_filters, activation='elu'))\nmodel.add(Dropout(0.2))\nmodel.add(GraphCNN(Y.shape[1], 2, graph_conv_filters))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.fit(X, Y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=500, shuffle=False)  Note on Example 2:  Equation  1  in the paper (see reference [3]) can be written as  Y=\\sum\\limits_{s=0}^{S}A^{(s)}X\\theta^{(s)} . This is defined as graph edge convolution. All we have to do is stack  A^{(s)}  and feed to GraphCNN layer to perform graph edge convolution.   [source]", 
            "title": "Example 2: Graph Edge Convolution for Node Classification"
        }, 
        {
            "location": "/Layers/Convolution/graph_conv_layer/#mutligraphcnn", 
            "text": "MutliGraphCNN(output_dim, num_filters, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  MutliGraphCNN assumes that the number of nodes for each graph in the dataset is same. For graph with arbitrary size, one can simply append appropriate zero rows or columns in adjacency matrix (and node feature matrix) based on max graph size in the dataset to achieve this uniformity.  Arguments   output_dim : Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).  num_filters : Positive integer, number of graph filters used for constructing   graph_conv_filters  input.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shapes   graph node feature matrix  input as a 3D tensor with shape:  (batch_size, num_graph_nodes, input_dim)  corresponding to graph node input feature matrix for each graph.  graph_conv_filters  input as a 3D tensor with shape:  (batch_size, num_filters*num_graph_nodes, num_graph_nodes)    num_filters  is different number of graph convolution filters to be applied on graph. For instance  num_filters  could be power of graph Laplacian.   Output shape   3D tensor with shape:  (batch_size, num_graph_nodes, output_dim)  representing convoluted output graph node embedding matrix for each graph in batch size.   [source]", 
            "title": "MutliGraphCNN"
        }, 
        {
            "location": "/Layers/Convolution/graph_conv_layer/#example-3-graph-classification", 
            "text": "# See multi_gcnn_graph_classification_example.py for complete code.\n\nfrom keras_dgl.layers import MultiGraphCNN\n\nX_shape = Input(shape=(X.shape[1], X.shape[2]))\ngraph_conv_filters_shape = Input(shape=(graph_conv_filters.shape[1], graph_conv_filters.shape[2]))\n\noutput = MultiGraphCNN(100, num_filters, activation='elu')([X_shape, graph_conv_filters_shape])\noutput = Dropout(0.2)(output)\noutput = MultiGraphCNN(100, num_filters, activation='elu')([output, graph_conv_filters_shape])\noutput = Dropout(0.2)(output)\noutput = Lambda(lambda x: K.mean(x, axis=1))(output)  # adding a node invariant layer to make sure output does not depends upon the node order in a graph.\noutput = Dense(Y.shape[1])(output)\noutput = Activation('softmax')(output)\n\nnb_epochs = 200\nbatch_size = 169\n\nmodel = Model(inputs=[X_shape, graph_conv_filters_shape], outputs=output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.fit([X, graph_conv_filters], Y, batch_size=batch_size, validation_split=0.1, epochs=nb_epochs, shuffle=True, verbose=1)", 
            "title": "Example 3: Graph Classification"
        }, 
        {
            "location": "/Layers/Convolution/graph_conv_layer/#remarks", 
            "text": "Why pass graph_conv_filters as a layer argument and not as an input in GraphCNN? \nThe problem lies with keras multi-input functional API. It requires --- all input arrays (x) should have the same number of samples i.e.,  all inputs first dimension axis should be same. In special cases the first dimension of inputs could be same, for example check out Kipf .et.al.  keras implementation  [source] . But in cases such as a graph recurrent neural networks this does not hold true.  Why pass graph_conv_filters as 2D tensor of this specific format? \nPassing  graph_conv_filters input as a 2D tensor with shape:  (K*num_graph_nodes, num_graph_nodes)  cut down few number of tensor computation operations.  References :  \n[1] Kipf, Thomas N., and Max Welling. \"Semi-supervised classification with graph convolutional networks.\" arXiv preprint arXiv:1609.02907 (2016).  \n[2] Defferrard, Micha\u00ebl, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional neural networks on graphs with fast localized spectral filtering.\" In Advances in Neural Information Processing Systems, pp. 3844-3852. 2016.  \n[3] Simonovsky, Martin, and Nikos Komodakis. \"Dynamic edge-conditioned filters in convolutional neural networks on graphs.\" In Proc. CVPR. 2017.", 
            "title": "Remarks"
        }, 
        {
            "location": "/Layers/Attention/graph_attention_layer/", 
            "text": "[source]\n\n\nGraphAttentionCNN\n\n\nGraphAttentionCNN(output_dim, adjacency_matrix, num_filters=None, graph_conv_filters=None, activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nGraphAttention layer assumes a fixed input graph structure which is passed as a layer argument. As a result, the input order of graph nodes are fixed for the model and should match the nodes order in inputs. Also, graph structure can not be changed once the model is compiled. This choice enable us to use Keras Sequential API but comes with some constraints (for instance shuffling is not  possible anymore in-or-after each epoch). See further \nremarks below\n about this specific choice.\n\n\nArguments\n\n\n\n\noutput_dim\n: Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).\n\n\nadjacency_matrix\n: input as a 2D tensor with shape: \n(num_graph_nodes, num_graph_nodes)\n with \ndiagonal values\n equal to 1.\n\n\nnum_filters\n: None or Positive integer, number of graph filters used for constructing  \ngraph_conv_filters\n input.\n\n\ngraph_conv_filters\n: None or input as a 2D tensor with shape: \n(num_filters*num_graph_nodes, num_graph_nodes)\n\n\nnum_filters\n is different number of graph convolution filters to be applied on graph. For instance \nnum_filters\n could be power of graph Laplacian. Here list of graph convolutional matrices are stacked along second-last axis.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector (recommended setting is False for this layer).\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shapes\n\n\n\n\n2D tensor with shape: \n(num_graph_nodes, input_dim)\n representing graph node input feature matrix.\n\n\n\n\nOutput shape\n\n\n\n\n2D tensor with shape: \n(num_graph_nodes, output_dim)\n representing convoluted output graph node embedding (or signal) matrix.\n\n\n\n\n[source]\n\n\nExample\n: Graph Semi-Supervised Learning (or Node Label Classification)\n\n\n# A complete example of applying GraphCNN layer for performing node label classification.\n\nmodel = Sequential()\nmodel.add(Dropout(0.6, input_shape=(X.shape[1],)))\nmodel.add(GraphAttentionCNN(8, 1, A, num_attention_heads=8, attention_heads_reduction='concat', attention_dropout=0.6, activation='elu', kernel_regularizer=l2(5e-4)))\nmodel.add(Dropout(0.6))\nmodel.add(GraphAttentionCNN(Y.shape[1], 1, A, num_attention_heads=1, attention_heads_reduction='average', attention_dropout=0.6, activation='elu', kernel_regularizer=l2(5e-4)))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=5e-3), metrics=['accuracy'])\n\nNB_EPOCH = 1000\n\nfor epoch in range(1, NB_EPOCH + 1):\n    model.fit(X, Y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n    Y_pred = model.predict(X, batch_size=A.shape[0])\n    _, train_acc = evaluate_preds(Y_pred, [Y_train], [train_idx])\n    _, test_acc = evaluate_preds(Y_pred, [Y_test], [test_idx])\n    print(\nEpoch: {:04d}\n.format(epoch), \ntrain_acc= {:.4f}\n.format(train_acc[0]),\n          \ntest_acc= {:.4f}\n.format(test_acc[0]))\n\n\n\n\n\n\n\n[source]\n\n\nMultiGraphAttentionCNN\n\n\nMutliGraphCNN(output_dim, num_filters, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\nMutliGraphCNN assumes that the number of nodes for each graph in the dataset is same. For graph with arbitrary size, one can simply append appropriate zero rows or columns in adjacency matrix (and node feature matrix) based on max graph size in the dataset to achieve this uniformity.\n\n\nArguments\n\n\n\n\noutput_dim\n: Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).\n\n\nnum_filters\n: Positive integer, number of graph filters used for constructing  \ngraph_conv_filters\n input.\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shapes\n\n\n\n\ngraph node feature matrix\n input as a 3D tensor with shape: \n(batch_size, num_graph_nodes, input_dim)\n corresponding to graph node input feature matrix for each graph.\n\n\ngraph_conv_filters\n input as a 3D tensor with shape: \n(batch_size, num_filters*num_graph_nodes, num_graph_nodes)\n \n\n\nnum_filters\n is different number of graph convolution filters to be applied on graph. For instance \nnum_filters\n could be power of graph Laplacian.\n\n\n\n\nOutput shape\n\n\n\n\n3D tensor with shape: \n(batch_size, num_graph_nodes, output_dim)\n representing convoluted output graph node embedding matrix for each graph in batch size.\n\n\n\n\n[source]\n\n\nExample 3\n: Graph Classification\n\n\n# See multi_graph_attention_cnn_graph_classification_example.py for complete code.\n\nfrom keras_dgl.layers import MultiAttentionGraphCNN\n\nX_input = Input(shape=(X.shape[1], X.shape[2]))\nA_input = Input(shape=(A.shape[1], A.shape[2]))\ngraph_conv_filters_input = Input(shape=(graph_conv_filters.shape[1], graph_conv_filters.shape[2]))\n\noutput = MultiGraphAttentionCNN(100, num_filters=num_filters, num_attention_heads=2, attention_combine='concat', attention_dropout=0.5, activation='elu', kernel_regularizer=l2(5e-4))([X_input, A_input, graph_conv_filters_input])\noutput = Dropout(0.2)(output)\noutput = MultiGraphAttentionCNN(100, num_filters=num_filters, num_attention_heads=1, attention_combine='average', attention_dropout=0.5, activation='elu', kernel_regularizer=l2(5e-4))([output, A_input, graph_conv_filters_input])\noutput = Dropout(0.2)(output)\noutput = Lambda(lambda x: K.mean(x, axis=1))(output)  # adding a node invariant layer to make sure output does not depends upon the node order in a graph.\noutput = Dense(Y.shape[1], activation='elu')(output)\noutput = Activation('softmax')(output)\n\nnb_epochs = 500\nbatch_size = 169\n\nmodel = Model(inputs=[X_input, A_input, graph_conv_filters_input], outputs=output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.fit([X, A, graph_conv_filters], Y, batch_size=batch_size, validation_split=0.1, epochs=nb_epochs, shuffle=True, verbose=1)", 
            "title": "Graph Attention Layers"
        }, 
        {
            "location": "/Layers/Attention/graph_attention_layer/#graphattentioncnn", 
            "text": "GraphAttentionCNN(output_dim, adjacency_matrix, num_filters=None, graph_conv_filters=None, activation=None, use_bias=False, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  GraphAttention layer assumes a fixed input graph structure which is passed as a layer argument. As a result, the input order of graph nodes are fixed for the model and should match the nodes order in inputs. Also, graph structure can not be changed once the model is compiled. This choice enable us to use Keras Sequential API but comes with some constraints (for instance shuffling is not  possible anymore in-or-after each epoch). See further  remarks below  about this specific choice.  Arguments   output_dim : Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).  adjacency_matrix : input as a 2D tensor with shape:  (num_graph_nodes, num_graph_nodes)  with  diagonal values  equal to 1.  num_filters : None or Positive integer, number of graph filters used for constructing   graph_conv_filters  input.  graph_conv_filters : None or input as a 2D tensor with shape:  (num_filters*num_graph_nodes, num_graph_nodes)  num_filters  is different number of graph convolution filters to be applied on graph. For instance  num_filters  could be power of graph Laplacian. Here list of graph convolutional matrices are stacked along second-last axis.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector (recommended setting is False for this layer).  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shapes   2D tensor with shape:  (num_graph_nodes, input_dim)  representing graph node input feature matrix.   Output shape   2D tensor with shape:  (num_graph_nodes, output_dim)  representing convoluted output graph node embedding (or signal) matrix.   [source]", 
            "title": "GraphAttentionCNN"
        }, 
        {
            "location": "/Layers/Attention/graph_attention_layer/#example-graph-semi-supervised-learning-or-node-label-classification", 
            "text": "# A complete example of applying GraphCNN layer for performing node label classification.\n\nmodel = Sequential()\nmodel.add(Dropout(0.6, input_shape=(X.shape[1],)))\nmodel.add(GraphAttentionCNN(8, 1, A, num_attention_heads=8, attention_heads_reduction='concat', attention_dropout=0.6, activation='elu', kernel_regularizer=l2(5e-4)))\nmodel.add(Dropout(0.6))\nmodel.add(GraphAttentionCNN(Y.shape[1], 1, A, num_attention_heads=1, attention_heads_reduction='average', attention_dropout=0.6, activation='elu', kernel_regularizer=l2(5e-4)))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=5e-3), metrics=['accuracy'])\n\nNB_EPOCH = 1000\n\nfor epoch in range(1, NB_EPOCH + 1):\n    model.fit(X, Y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=1, shuffle=False, verbose=0)\n    Y_pred = model.predict(X, batch_size=A.shape[0])\n    _, train_acc = evaluate_preds(Y_pred, [Y_train], [train_idx])\n    _, test_acc = evaluate_preds(Y_pred, [Y_test], [test_idx])\n    print( Epoch: {:04d} .format(epoch),  train_acc= {:.4f} .format(train_acc[0]),\n           test_acc= {:.4f} .format(test_acc[0]))   [source]", 
            "title": "Example: Graph Semi-Supervised Learning (or Node Label Classification)"
        }, 
        {
            "location": "/Layers/Attention/graph_attention_layer/#multigraphattentioncnn", 
            "text": "MutliGraphCNN(output_dim, num_filters, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  MutliGraphCNN assumes that the number of nodes for each graph in the dataset is same. For graph with arbitrary size, one can simply append appropriate zero rows or columns in adjacency matrix (and node feature matrix) based on max graph size in the dataset to achieve this uniformity.  Arguments   output_dim : Positive integer, dimensionality of each graph node feature output space (or also referred dimension of graph node embedding).  num_filters : Positive integer, number of graph filters used for constructing   graph_conv_filters  input.  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shapes   graph node feature matrix  input as a 3D tensor with shape:  (batch_size, num_graph_nodes, input_dim)  corresponding to graph node input feature matrix for each graph.  graph_conv_filters  input as a 3D tensor with shape:  (batch_size, num_filters*num_graph_nodes, num_graph_nodes)    num_filters  is different number of graph convolution filters to be applied on graph. For instance  num_filters  could be power of graph Laplacian.   Output shape   3D tensor with shape:  (batch_size, num_graph_nodes, output_dim)  representing convoluted output graph node embedding matrix for each graph in batch size.   [source]", 
            "title": "MultiGraphAttentionCNN"
        }, 
        {
            "location": "/Layers/Attention/graph_attention_layer/#example-3-graph-classification", 
            "text": "# See multi_graph_attention_cnn_graph_classification_example.py for complete code.\n\nfrom keras_dgl.layers import MultiAttentionGraphCNN\n\nX_input = Input(shape=(X.shape[1], X.shape[2]))\nA_input = Input(shape=(A.shape[1], A.shape[2]))\ngraph_conv_filters_input = Input(shape=(graph_conv_filters.shape[1], graph_conv_filters.shape[2]))\n\noutput = MultiGraphAttentionCNN(100, num_filters=num_filters, num_attention_heads=2, attention_combine='concat', attention_dropout=0.5, activation='elu', kernel_regularizer=l2(5e-4))([X_input, A_input, graph_conv_filters_input])\noutput = Dropout(0.2)(output)\noutput = MultiGraphAttentionCNN(100, num_filters=num_filters, num_attention_heads=1, attention_combine='average', attention_dropout=0.5, activation='elu', kernel_regularizer=l2(5e-4))([output, A_input, graph_conv_filters_input])\noutput = Dropout(0.2)(output)\noutput = Lambda(lambda x: K.mean(x, axis=1))(output)  # adding a node invariant layer to make sure output does not depends upon the node order in a graph.\noutput = Dense(Y.shape[1], activation='elu')(output)\noutput = Activation('softmax')(output)\n\nnb_epochs = 500\nbatch_size = 169\n\nmodel = Model(inputs=[X_input, A_input, graph_conv_filters_input], outputs=output)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nmodel.fit([X, A, graph_conv_filters], Y, batch_size=batch_size, validation_split=0.1, epochs=nb_epochs, shuffle=True, verbose=1)", 
            "title": "Example 3: Graph Classification"
        }, 
        {
            "location": "/Layers/Recurrent/graph_conv_recurrent_layer/", 
            "text": "[source]\n\n\nGraphConvLSTM\n\n\nGraphConvLSTM(output_dim, graph_conv_filters, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n\n\n\n\n1D convolution layer (e.g. temporal convolution).\n\n\nThis layer creates a convolution kernel that is convolved\nwith the layer input over a single spatial (or temporal) dimension\nto produce a tensor of outputs.\nIf \nuse_bias\n is True, a bias vector is created and added to the outputs.\nFinally, if \nactivation\n is not \nNone\n,\nit is applied to the outputs as well.\n\n\nWhen using this layer as the first layer in a model,\nprovide an \ninput_shape\n argument\n(tuple of integers or \nNone\n, e.g.\n\n(10, 128)\n for sequences of 10 vectors of 128-dimensional vectors,\nor \n(None, 128)\n for variable-length sequences of 128-dimensional vectors.\n\n\nArguments\n\n\n\n\noutput_dim\n: Positive integer, dimensionality of each graph node output space (or dimension of graph node embedding).\n\n\ngraph_conv_filters\n: 3D Tensor, the dimensionality of the output space\n(i.e. the number output of filters in the convolution).\n\n\nactivation\n: Activation function to use\n(see \nactivations\n).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation: \na(x) = x\n).\n\n\nuse_bias\n: Boolean, whether the layer uses a bias vector.\n\n\nkernel_initializer\n: Initializer for the \nkernel\n weights matrix\n(see \ninitializers\n).\n\n\nbias_initializer\n: Initializer for the bias vector\n(see \ninitializers\n).\n\n\nkernel_regularizer\n: Regularizer function applied to\nthe \nkernel\n weights matrix\n(see \nregularizer\n).\n\n\nbias_regularizer\n: Regularizer function applied to the bias vector\n(see \nregularizer\n).\n\n\nactivity_regularizer\n: Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see \nregularizer\n).\n\n\nkernel_constraint\n: Constraint function applied to the kernel matrix\n(see \nconstraints\n).\n\n\nbias_constraint\n: Constraint function applied to the bias vector\n(see \nconstraints\n).\n\n\n\n\nInput shapes\n\n\n\n\n4D tensor with shape: \n(samples, timestep, num_graph_nodes, input_dim)\n\n\n\n\nOutput shape\n\n\n\n\nif \nreturn_sequences\n\n4D tensor with shape: \n(samples, timestep, num_graph_nodes, output_dim)\n\n\nelse\n\n4D tensor with shape: \n(samples, num_graph_nodes, output_dim)", 
            "title": "Graph Recurrent Layers"
        }, 
        {
            "location": "/Layers/Recurrent/graph_conv_recurrent_layer/#graphconvlstm", 
            "text": "GraphConvLSTM(output_dim, graph_conv_filters, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)  1D convolution layer (e.g. temporal convolution).  This layer creates a convolution kernel that is convolved\nwith the layer input over a single spatial (or temporal) dimension\nto produce a tensor of outputs.\nIf  use_bias  is True, a bias vector is created and added to the outputs.\nFinally, if  activation  is not  None ,\nit is applied to the outputs as well.  When using this layer as the first layer in a model,\nprovide an  input_shape  argument\n(tuple of integers or  None , e.g. (10, 128)  for sequences of 10 vectors of 128-dimensional vectors,\nor  (None, 128)  for variable-length sequences of 128-dimensional vectors.  Arguments   output_dim : Positive integer, dimensionality of each graph node output space (or dimension of graph node embedding).  graph_conv_filters : 3D Tensor, the dimensionality of the output space\n(i.e. the number output of filters in the convolution).  activation : Activation function to use\n(see  activations ).\nIf you don't specify anything, no activation is applied\n(ie. \"linear\" activation:  a(x) = x ).  use_bias : Boolean, whether the layer uses a bias vector.  kernel_initializer : Initializer for the  kernel  weights matrix\n(see  initializers ).  bias_initializer : Initializer for the bias vector\n(see  initializers ).  kernel_regularizer : Regularizer function applied to\nthe  kernel  weights matrix\n(see  regularizer ).  bias_regularizer : Regularizer function applied to the bias vector\n(see  regularizer ).  activity_regularizer : Regularizer function applied to\nthe output of the layer (its \"activation\").\n(see  regularizer ).  kernel_constraint : Constraint function applied to the kernel matrix\n(see  constraints ).  bias_constraint : Constraint function applied to the bias vector\n(see  constraints ).   Input shapes   4D tensor with shape:  (samples, timestep, num_graph_nodes, input_dim)   Output shape   if  return_sequences \n4D tensor with shape:  (samples, timestep, num_graph_nodes, output_dim)  else \n4D tensor with shape:  (samples, num_graph_nodes, output_dim)", 
            "title": "GraphConvLSTM"
        }, 
        {
            "location": "/Layers/Graph Capsule Neural Network/graph_capsule_cnn/", 
            "text": "TBD", 
            "title": "Graph Capsule CNN Layers"
        }, 
        {
            "location": "/Layers/Graph Neural Network/graph_neural_networks/", 
            "text": "TBD", 
            "title": "Graph Neural Network Layers"
        }, 
        {
            "location": "/Filters/graph_conv_filters/", 
            "text": "Standard Polynomial\n\n\nstandard_poly(A, poly_degree=3)\n\n\n\n\nComputes Standard Polynomial function. Current implementation complexity is \nO(N^2)\n.\n\n\nInputs\n:\n\n\n\n\nA\n : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.\n\n\npoly_degree\n: Integer, polynomial degree (default=1). \n\n\n\n\nOutputs\n:\n\n\n\n\n3D Tensor, containing standard polynomial powers of graph adjacency matrix or Laplacian.\n\n\n\n\n\n\nChebyshev Polynomial\n\n\nchebyshev_poly(A, poly_degree=3)\n\n\n\n\nComputes Chebyshev Polynomial function. Current implementation complexity is \nO(N^2)\n.\n\n\nInputs\n:\n\n\n\n\nA\n : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.\n\n\npoly_degree\n: Integer, polynomial degree (default=1). \n\n\n\n\nOutputs\n:\n\n\n\n\n3D Tensor, containing chebyshev polynomial powers of graph adjacency matrix or Laplacian.\n\n\n\n\nReferences\n: Defferrard, Micha\u00ebl, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional neural networks on graphs with fast localized spectral filtering.\" In Advances in Neural Information Processing Systems, pp. 3844-3852. 2016.\n\n\n\n\nRandom Walk Polynomial\n\n\nchebyshev_poly(A, poly_degree=3)\n\n\n\n\nComputes Random Walk Polynomial function. Current implementation complexity is \nO(N^2)\n.\n\n\nInputs\n:\n\n\n\n\nA\n : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.\n\n\npoly_degree\n: Integer, polynomial degree (default=1). \n\n\n\n\nOutputs\n:\n\n\n\n\n3D Tensor, containing chebyshev polynomial powers of graph adjacency matrix or Laplacian.\n\n\n\n\n\n\nCayley Polynomial\n\n\ncayley_poly(A, poly_degree=3)\n\n\n\n\nComputes Cayley Polynomial function. Current implementation complexity is \nO(N^3)\n.\n\n\nInputs\n:\n\n\n\n\nA\n : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.\n\n\npoly_degree\n: Integer, polynomial degree (default=1). \n\n\n\n\nOutputs\n:\n\n\n\n\n3D Tensor, containing cayley polynomial powers of graph adjacency matrix or Laplacian.\n\n\n\n\nReferences\n: Levie, Ron, Federico Monti, Xavier Bresson, and Michael M. Bronstein. \"Cayleynets: Graph convolutional neural networks with complex rational spectral filters.\" arXiv preprint arXiv:1705.07664 (2017).\n\n\n\n\nCombine Polynomial\n\n\ncombine_poly(A, B, poly_degree=3)\n\n\n\n\nComputes combination of polynomial function.\n\n\nInputs\n:\n\n\n\n\nA\n : 2D Tensor, graph adjacency or (normalized) Laplacian or cayley matrix.\n\n\nB\n : 2D Tensor, graph adjacency matrix or (normalized) Laplacian or cayley matrix.\n\n\npoly_degree\n: Integer, polynomial degree (default=1). \n\n\n\n\nOutputs\n:\n\n\n\n\n3D Tensor, containing combine polynomial powers of graph adjacency  or Laplacian or cayley matrix.", 
            "title": "Graph Convolution Filters"
        }, 
        {
            "location": "/Filters/graph_conv_filters/#standard-polynomial", 
            "text": "standard_poly(A, poly_degree=3)  Computes Standard Polynomial function. Current implementation complexity is  O(N^2) .  Inputs :   A  : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.  poly_degree : Integer, polynomial degree (default=1).    Outputs :   3D Tensor, containing standard polynomial powers of graph adjacency matrix or Laplacian.", 
            "title": "Standard Polynomial"
        }, 
        {
            "location": "/Filters/graph_conv_filters/#chebyshev-polynomial", 
            "text": "chebyshev_poly(A, poly_degree=3)  Computes Chebyshev Polynomial function. Current implementation complexity is  O(N^2) .  Inputs :   A  : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.  poly_degree : Integer, polynomial degree (default=1).    Outputs :   3D Tensor, containing chebyshev polynomial powers of graph adjacency matrix or Laplacian.   References : Defferrard, Micha\u00ebl, Xavier Bresson, and Pierre Vandergheynst. \"Convolutional neural networks on graphs with fast localized spectral filtering.\" In Advances in Neural Information Processing Systems, pp. 3844-3852. 2016.", 
            "title": "Chebyshev Polynomial"
        }, 
        {
            "location": "/Filters/graph_conv_filters/#random-walk-polynomial", 
            "text": "chebyshev_poly(A, poly_degree=3)  Computes Random Walk Polynomial function. Current implementation complexity is  O(N^2) .  Inputs :   A  : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.  poly_degree : Integer, polynomial degree (default=1).    Outputs :   3D Tensor, containing chebyshev polynomial powers of graph adjacency matrix or Laplacian.", 
            "title": "Random Walk Polynomial"
        }, 
        {
            "location": "/Filters/graph_conv_filters/#cayley-polynomial", 
            "text": "cayley_poly(A, poly_degree=3)  Computes Cayley Polynomial function. Current implementation complexity is  O(N^3) .  Inputs :   A  : 2D Tensor, graph adjacency matrix or (normalized) Laplacian.  poly_degree : Integer, polynomial degree (default=1).    Outputs :   3D Tensor, containing cayley polynomial powers of graph adjacency matrix or Laplacian.   References : Levie, Ron, Federico Monti, Xavier Bresson, and Michael M. Bronstein. \"Cayleynets: Graph convolutional neural networks with complex rational spectral filters.\" arXiv preprint arXiv:1705.07664 (2017).", 
            "title": "Cayley Polynomial"
        }, 
        {
            "location": "/Filters/graph_conv_filters/#combine-polynomial", 
            "text": "combine_poly(A, B, poly_degree=3)  Computes combination of polynomial function.  Inputs :   A  : 2D Tensor, graph adjacency or (normalized) Laplacian or cayley matrix.  B  : 2D Tensor, graph adjacency matrix or (normalized) Laplacian or cayley matrix.  poly_degree : Integer, polynomial degree (default=1).    Outputs :   3D Tensor, containing combine polynomial powers of graph adjacency  or Laplacian or cayley matrix.", 
            "title": "Combine Polynomial"
        }, 
        {
            "location": "/about/", 
            "text": "Author\n\n\nSaurabh Verma, PhD Student at University of Minnesota Twin Cities.\n\n\nAbout me: \nhttp://www-users.cs.umn.edu/~verma076/\n\n\nAcknowledgments\n: Some part of the code has been borrowed from the original authors implementation and others. List of these authors are TBD.", 
            "title": "About"
        }, 
        {
            "location": "/about/#author", 
            "text": "Saurabh Verma, PhD Student at University of Minnesota Twin Cities.  About me:  http://www-users.cs.umn.edu/~verma076/  Acknowledgments : Some part of the code has been borrowed from the original authors implementation and others. List of these authors are TBD.", 
            "title": "Author"
        }
    ]
}